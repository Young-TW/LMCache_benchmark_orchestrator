import os
import time
import subprocess
import shutil
import requests
import yaml
import argparse
from pathlib import Path
from copy import deepcopy

# ================= è·¯å¾‘èˆ‡ç’°å¢ƒé…ç½® =================

# 1. å‹•æ…‹å®šä½ src ç›®éŒ„èˆ‡å°ˆæ¡ˆæ ¹ç›®éŒ„
CURRENT_FILE = Path(__file__).resolve()
SRC_DIR = CURRENT_FILE.parent           # .../LMCache_benchmark_orchestrator/src
PROJECT_ROOT = SRC_DIR.parent           # .../LMCache_benchmark_orchestrator

# 2. å®šç¾©ç”¢å‡ºç›®éŒ„ (æ‰€æœ‰ç”Ÿæˆçš„æª”æ¡ˆéƒ½æ”¾åœ¨ runs è³‡æ–™å¤¾)
RUNS_DIR = PROJECT_ROOT / "runs"

# 3. æ¨¡å‹è·¯å¾‘ (å„ªå…ˆè®€å–ç’°å¢ƒè®Šæ•¸ï¼Œå¦å‰‡ä½¿ç”¨é è¨­å€¼)
# ä½¿ç”¨æ–¹æ³•: export LLM_MODELS_DIR="/path/to/your/models"
MODELS_DIR = os.getenv("LLM_MODELS_DIR", "/home/young/models")

# 4. æ¸¬è©¦è…³æœ¬ä½ç½®
TESTER_SCRIPT = SRC_DIR / "latency_tester.py"

print(f"å°ˆæ¡ˆæ ¹ç›®éŒ„: {PROJECT_ROOT}")
print(f"æ¨¡å‹ä¾†æºè·¯å¾‘: {MODELS_DIR}")
print(f"æ¸¬è©¦å·¥ä½œå€: {RUNS_DIR}")

# ================= æ¸¬è©¦çŸ©é™£ =================
# åœ¨é€™è£¡å®šç¾©æ‚¨çš„å„ç¨®çµ„åˆ
TEST_MATRIX = [
    {
        "id": "1p7d_llama3_70b",
        "model_rel_path": "Llama-3.3-70B-Instruct", # ç›¸å°æ–¼ MODELS_DIR çš„è·¯å¾‘
        "type": "disaggregated",
        "producers": 1,
        "consumers": 7,
        "tp_per_instance": 1,
        "gpu_offset": 0
    },
    # æ‚¨å¯ä»¥åœ¨æ­¤åŠ å…¥æ›´å¤šçµ„åˆ (å¦‚ 2p6d, tp8_baseline ç­‰)
]

# é€šç”¨å®¹å™¨ç’°å¢ƒè®Šæ•¸
COMMON_ENV = {
    "HF_HOME": "/app/model",
    "PYTORCH_ROCM_ARCH": "gfx942",
    "TORCH_DONT_CHECK_COMPILER_ABI": "1",
    "CXX": "hipcc",
    "BUILD_WITH_HIP": "1",
    "LMCACHE_CONFIG_FILE": "/app/lmcache_config.yaml",
    "VLLM_WORKER_MULTIPROC_METHOD": "spawn",
    "PYTHONHASHSEED": "0"
}

def generate_docker_compose(config, work_dir):
    """
    å‹•æ…‹ç”Ÿæˆ docker-compose.yaml
    work_dir: è©²æ¬¡æ¸¬è©¦çš„å°ˆå±¬ç›®éŒ„ (ä¾‹å¦‚ runs/1p7d_llama3_70b)
    """

    services = {}
    full_model_path = Path(MODELS_DIR) / config["model_rel_path"]

    # æª¢æŸ¥æ¨¡å‹è·¯å¾‘æ˜¯å¦å­˜åœ¨
    if not full_model_path.exists():
        print(f"âš ï¸ è­¦å‘Š: æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {full_model_path}")

    # LMCache Redis
    if config["type"] == "disaggregated":
        services["redis"] = {
            "image": "bitnamilegacy/redis:7.4.2-debian-12-r6",
            "container_name": f"lmcache_redis_{config['id']}",
            "network_mode": "host",
            "command": 'redis-server --save "" --appendonly no'
        }

    # vLLM Template
    vllm_template = {
        "image": "rocm/vllm-dev:nightly_main_20260112",
        "network_mode": "host",
        "group_add": ["video"],
        "cap_add": ["SYS_PTRACE"],
        "security_opt": ["seccomp:unconfined"],
        "devices": ["/dev/kfd:/dev/kfd", "/dev/dri:/dev/dri"],
        "volumes": [
            f"{full_model_path}:/app/model",
            "./lmcache_config.yaml:/app/lmcache_config.yaml",
            f"/dev/shm/lmcache_{config['id']}:/dev/shm/lmcache_store"
        ],
        "environment": deepcopy(COMMON_ENV)
    }

    current_gpu_idx = config["gpu_offset"]
    base_port = 8000

    # å»ºç«‹ Producers
    if config["type"] == "disaggregated":
        for i in range(config["producers"]):
            s_name = f"producer_{i}"
            svc = deepcopy(vllm_template)
            svc["container_name"] = f"lmcache_{config['id']}_p{i}"

            gpus = ",".join([str(x) for x in range(current_gpu_idx, current_gpu_idx + config["tp_per_instance"])])
            current_gpu_idx += config["tp_per_instance"]

            svc["environment"]["CUDA_VISIBLE_DEVICES"] = gpus

            kv_config = '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_producer"}'
            cmd = f"""python3 -m vllm.entrypoints.openai.api_server
            --model /app/model
            --port {base_port}
            --tensor-parallel-size {config['tp_per_instance']}
            --max-model-len 8192
            --kv-transfer-config '{kv_config}'"""

            svc["command"] = "bash -c '" + cmd.replace("\n", " ") + "'"
            svc["depends_on"] = ["redis"]
            services[s_name] = svc
            base_port += 1

    # å»ºç«‹ Consumers (æˆ–æ˜¯ Standalone)
    num_consumers = config["consumers"]
    for i in range(num_consumers):
        s_name = f"consumer_{i}" if config["type"] == "disaggregated" else "vllm_standalone"
        svc = deepcopy(vllm_template)
        svc["container_name"] = f"lmcache_{config['id']}_c{i}"

        gpus = ",".join([str(x) for x in range(current_gpu_idx, current_gpu_idx + config["tp_per_instance"])])
        current_gpu_idx += config["tp_per_instance"]

        svc["environment"]["CUDA_VISIBLE_DEVICES"] = gpus

        kv_arg = ""
        if config["type"] == "disaggregated":
             kv_arg = "--kv-transfer-config '{\"kv_connector\":\"LMCacheConnectorV1\", \"kv_role\":\"kv_consumer\"}'"
             svc["depends_on"] = ["redis"]

        cmd = f"""python3 -m vllm.entrypoints.openai.api_server
        --model /app/model
        --port {base_port}
        --tensor-parallel-size {config['tp_per_instance']}
        --max-model-len 8192
        {kv_arg}"""

        svc["command"] = "bash -c '" + cmd.replace("\n", " ") + "'"
        services[s_name] = svc
        base_port += 1

    # å¯«å…¥ docker-compose.yaml
    compose_data = {"version": "3.8", "services": services}
    with open(work_dir / "docker-compose.yaml", "w") as f:
        yaml.dump(compose_data, f)

    # å¯«å…¥ lmcache_config.yaml
    lmcache_conf = """
chunk_size: 256
local_device: "cpu"
remote_url: "redis://localhost:6379"
remote_serde: "cachegen"
    """
    with open(work_dir / "lmcache_config.yaml", "w") as f:
        f.write(lmcache_conf)

    return True

def wait_for_services(ports, timeout=900):
    """æª¢æŸ¥æ‰€æœ‰ API æ˜¯å¦å­˜æ´»"""
    print(f"â³ ç­‰å¾…æœå‹™å•Ÿå‹•ï¼Œç›®æ¨™ Ports: {ports}")
    start_time = time.time()
    pending_ports = set(ports)

    while pending_ports:
        if time.time() - start_time > timeout:
            print(f"âŒ é€¾æ™‚ï¼ç„¡æ³•å•Ÿå‹•çš„ Ports: {pending_ports}")
            return False

        for port in list(pending_ports):
            try:
                requests.get(f"http://localhost:{port}/v1/models", timeout=2)
                print(f"âœ… Port {port} å·²å°±ç·’")
                pending_ports.remove(port)
            except:
                pass

        if pending_ports:
            time.sleep(10)

    return True

def run_single_benchmark(config):
    test_id = config["id"]
    work_dir = RUNS_DIR / test_id

    # æ¸…ç†ä¸¦é‡å»ºå·¥ä½œç›®éŒ„
    if work_dir.exists():
        shutil.rmtree(work_dir)
    work_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n================ é–‹å§‹æ¸¬è©¦: {test_id} ================")
    generate_docker_compose(config, work_dir)

    try:
        # å•Ÿå‹•ç’°å¢ƒ
        print(f"ğŸš€ å•Ÿå‹• Docker ç’°å¢ƒ (Dir: {work_dir})...")
        subprocess.run(["docker", "compose", "up", "-d"], cwd=work_dir, check=True)

        # è¨ˆç®— Ports
        start_port = 8000
        producer_ports = []
        consumer_ports = []

        if config["type"] == "disaggregated":
            for _ in range(config["producers"]):
                producer_ports.append(start_port)
                start_port += 1
            for _ in range(config["consumers"]):
                consumer_ports.append(start_port)
                start_port += 1
        else:
            consumer_ports.append(start_port)
            start_port += 1

        all_ports = producer_ports + consumer_ports

        # ç­‰å¾…ä¸¦åŸ·è¡Œæ¸¬è©¦
        if wait_for_services(all_ports):
            p_urls = ",".join([f"http://localhost:{p}/v1" for p in producer_ports])
            c_urls = ",".join([f"http://localhost:{p}/v1" for p in consumer_ports])

            # å‘¼å«æ¸¬è©¦è…³æœ¬ï¼Œä¸¦æŒ‡å®šè¼¸å‡ºç›®éŒ„
            cmd = [
                "uv", "run", str(TESTER_SCRIPT),
                "--test-id", test_id,
                "--producers", p_urls,
                "--consumers", c_urls,
                "--output-dir", str(work_dir) # å°‡çµæœå­˜åœ¨å°æ‡‰çš„å·¥ä½œç›®éŒ„
            ]

            print(f"ğŸ§ª åŸ·è¡Œæ¸¬è©¦è…³æœ¬...")
            subprocess.run(cmd, check=True)
        else:
            print("âš ï¸ æ¸¬è©¦è·³éï¼šæœå‹™å•Ÿå‹•å¤±æ•—")

    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
    finally:
        # æ¸…ç†ç’°å¢ƒ
        print(f"ğŸ§¹ æ­£åœ¨æ¸…ç† {test_id}...")
        subprocess.run(["docker", "compose", "down"], cwd=work_dir)

        # æ¸…ç† SHM (é‡è¦)
        shm_path = Path(f"/dev/shm/lmcache_{test_id}")
        if shm_path.exists():
            shutil.rmtree(shm_path, ignore_errors=True)

if __name__ == "__main__":
    if not MODELS_DIR or not Path(MODELS_DIR).exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ¨¡å‹ç›®éŒ„ {MODELS_DIR} ä¸å­˜åœ¨ã€‚")
        print("è«‹è¨­å®šç’°å¢ƒè®Šæ•¸: export LLM_MODELS_DIR='/path/to/models'")
        exit(1)

    for config in TEST_MATRIX:
        run_single_benchmark(config)
        time.sleep(5)