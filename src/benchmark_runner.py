import os
import time
import subprocess
import shutil
import requests
import yaml
import argparse
from pathlib import Path
from copy import deepcopy

# ================= è·¯å¾‘èˆ‡ç’°å¢ƒé…ç½® =================

CURRENT_FILE = Path(__file__).resolve()
SRC_DIR = CURRENT_FILE.parent
PROJECT_ROOT = SRC_DIR.parent
RUNS_DIR = PROJECT_ROOT / "runs"
MODELS_DIR = os.getenv("LLM_MODELS_DIR", "/home/young/models")
TESTER_SCRIPT = SRC_DIR / "latency_tester.py"

# LMCache Source Config
LMCACHE_REPO = "https://github.com/Young-TW/LMCache.git"
LMCACHE_COMMIT = "505cd45d494d976acaf8d26e5aa598f30a6ea790"
LMCACHE_SRC_DIR = PROJECT_ROOT / "LMCache_src"

print(f"å°ˆæ¡ˆæ ¹ç›®éŒ„: {PROJECT_ROOT}")
print(f"æ¨¡å‹ä¾†æºè·¯å¾‘: {MODELS_DIR}")
print(f"LMCache åŸå§‹ç¢¼: {LMCACHE_SRC_DIR}")

# ================= æ¸¬è©¦çŸ©é™£ =================
TEST_MATRIX = [
    {
        "id": "1p7d_llama3_70b",
        "model_rel_path": "Llama-3.3-70B-Instruct",
        "type": "disaggregated",
        "producers": 1,
        "consumers": 7,
        "tp_per_instance": 1,
        "gpu_offset": 0
    },
]

COMMON_ENV = {
    "HF_HOME": "/app/model",
    "PYTORCH_ROCM_ARCH": "gfx942",
    "TORCH_DONT_CHECK_COMPILER_ABI": "1",
    "CXX": "hipcc",
    "BUILD_WITH_HIP": "1",
    "LMCACHE_CONFIG_FILE": "/app/lmcache_config.yaml",
    "VLLM_WORKER_MULTIPROC_METHOD": "spawn",
    "PYTHONHASHSEED": "0"
}

def prepare_lmcache_source():
    """Clone LMCache repo and checkout specific commit on host."""
    # 1. Clone
    if not LMCACHE_SRC_DIR.exists():
        print(f"ğŸ“¥ Cloning LMCache from {LMCACHE_REPO}...")
        subprocess.run(["git", "clone", LMCACHE_REPO, str(LMCACHE_SRC_DIR)], check=True)

    # 2. Checkout
    print(f"ğŸ”„ Checking out commit {LMCACHE_COMMIT}...")
    subprocess.run(["git", "fetch", "--all"], cwd=LMCACHE_SRC_DIR, check=True)
    subprocess.run(["git", "checkout", LMCACHE_COMMIT], cwd=LMCACHE_SRC_DIR, check=True)
    print("âœ… Source code prepared.")

def generate_docker_compose(config, work_dir):
    """
    Generate docker-compose.yaml using --no-build-isolation for pip
    """
    services = {}
    port_map = {}

    full_model_path = Path(MODELS_DIR) / config["model_rel_path"]
    if not full_model_path.exists():
        print(f"âš ï¸ è­¦å‘Š: æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {full_model_path}")

    # Redis
    if config["type"] == "disaggregated":
        services["redis"] = {
            "image": "bitnamilegacy/redis:7.4.2-debian-12-r6",
            "container_name": f"lmcache_redis_{config['id']}",
            "network_mode": "host",
            "command": 'redis-server --save "" --appendonly no'
        }

    vllm_template = {
        "image": "rocm/vllm-dev:nightly_main_20260112",
        "network_mode": "host",
        "group_add": ["video"],
        "cap_add": ["SYS_PTRACE"],
        "security_opt": ["seccomp:unconfined"],
        "devices": ["/dev/kfd:/dev/kfd", "/dev/dri:/dev/dri"],
        "volumes": [
            f"{full_model_path}:/app/model",
            "./lmcache_config.yaml:/app/lmcache_config.yaml",
            f"/dev/shm/lmcache_{config['id']}:/dev/shm/lmcache_store",
            # Mount LMCache source code
            f"{LMCACHE_SRC_DIR}:/app/LMCache_src"
        ],
        "environment": deepcopy(COMMON_ENV)
    }

    current_gpu_idx = config["gpu_offset"]
    base_port = 8000

    def get_kv_config(role):
        return f'\\"kv_connector\\":\\"LMCacheConnectorV1\\", \\"kv_role\\":\\"{role}\\"'

    # Helper to construct command
    def build_command(role_json_content, port, gpu_count):
        kv_json = "{" + role_json_content + "}"

        # [é—œéµä¿®æ­£] åŠ å…¥ --no-build-isolation
        # é€™æ¨£ pip å°±æœƒä½¿ç”¨å®¹å™¨å…§åŸæœ¬å·²ç¶“è£å¥½çš„ ROCm PyTorchï¼Œè€Œä¸æœƒå˜—è©¦å»ä¸‹è¼‰ä¸ç›¸å®¹çš„ç‰ˆæœ¬
        install_cmd = "python3 -m pip install --no-build-isolation -e /app/LMCache_src &&"

        vllm_cmd = f"""python3 -m vllm.entrypoints.openai.api_server
        --model /app/model --port {port} --tensor-parallel-size {gpu_count}
        --max-model-len 8192 --kv-transfer-config "{kv_json}" """

        full_cmd = f"{install_cmd} {vllm_cmd}"
        return "bash -c '" + full_cmd.replace("\n", " ") + "'"

    # Producers
    if config["type"] == "disaggregated":
        for i in range(config["producers"]):
            container_name = f"lmcache_{config['id']}_p{i}"
            s_name = f"producer_{i}"

            svc = deepcopy(vllm_template)
            svc["container_name"] = container_name

            gpus = ",".join([str(x) for x in range(current_gpu_idx, current_gpu_idx + config["tp_per_instance"])])
            current_gpu_idx += config["tp_per_instance"]

            svc["environment"]["CUDA_VISIBLE_DEVICES"] = gpus

            svc["command"] = build_command(
                get_kv_config("kv_producer"),
                base_port,
                config['tp_per_instance']
            )
            svc["depends_on"] = ["redis"]

            services[s_name] = svc
            port_map[base_port] = container_name
            base_port += 1

    # Consumers
    num_consumers = config["consumers"]
    for i in range(num_consumers):
        container_name = f"lmcache_{config['id']}_c{i}" if config["type"] == "disaggregated" else "vllm_standalone"
        s_name = f"consumer_{i}" if config["type"] == "disaggregated" else "vllm_standalone"

        svc = deepcopy(vllm_template)
        svc["container_name"] = container_name

        gpus = ",".join([str(x) for x in range(current_gpu_idx, current_gpu_idx + config["tp_per_instance"])])
        current_gpu_idx += config["tp_per_instance"]

        svc["environment"]["CUDA_VISIBLE_DEVICES"] = gpus

        kv_role_config = ""
        if config["type"] == "disaggregated":
             kv_role_config = get_kv_config("kv_consumer")
             svc["depends_on"] = ["redis"]
             svc["command"] = build_command(kv_role_config, base_port, config['tp_per_instance'])
        else:
             cmd = f"""python3 -m vllm.entrypoints.openai.api_server
             --model /app/model --port {base_port} --tensor-parallel-size {config['tp_per_instance']}
             --max-model-len 8192"""
             svc["command"] = "bash -c '" + cmd.replace("\n", " ") + "'"

        services[s_name] = svc
        port_map[base_port] = container_name
        base_port += 1

    # Output Files
    with open(work_dir / "docker-compose.yaml", "w") as f:
        yaml.dump({"version": "3.8", "services": services}, f)

    lmcache_conf = """
chunk_size: 256
local_device: "cpu"
remote_url: "redis://localhost:6379"
remote_serde: "cachegen"
    """
    with open(work_dir / "lmcache_config.yaml", "w") as f:
        f.write(lmcache_conf)

    return port_map

def print_container_logs(container_name):
    print(f"\nğŸ”´ [DEBUG] Dump Log for: {container_name}")
    print("=" * 60)
    try:
        subprocess.run(["docker", "logs", "--tail", "50", container_name], check=False)
    except Exception as e:
        print(f"ç„¡æ³•å–å¾— Log: {e}")
    print("=" * 60 + "\n")

def wait_for_services(port_map, timeout=900):
    ports = list(port_map.keys())
    print(f"â³ ç­‰å¾…æœå‹™å•Ÿå‹• (Timeout: {timeout}s)... ç›®æ¨™ Ports: {ports}")

    start_time = time.time()
    pending_ports = set(ports)

    while pending_ports:
        if time.time() - start_time > timeout:
            print(f"\nâŒ å•Ÿå‹•é€¾æ™‚ï¼")
            for p in pending_ports:
                c_name = port_map.get(p, "unknown")
                print_container_logs(c_name)
            return False

        for port in list(pending_ports):
            try:
                requests.get(f"http://localhost:{port}/v1/models", timeout=2)
                print(f"âœ… Port {port} å·²å°±ç·’")
                pending_ports.remove(port)
            except:
                pass

        if pending_ports:
            time.sleep(10)
            elapsed = int(time.time() - start_time)
            if elapsed % 30 == 0:
                print(f"   ...å·²ç­‰å¾… {elapsed}sï¼Œå‰©é¤˜: {len(pending_ports)}")

    return True

def run_single_benchmark(config):
    test_id = config["id"]
    work_dir = RUNS_DIR / test_id

    if work_dir.exists():
        shutil.rmtree(work_dir)
    work_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n================ é–‹å§‹æ¸¬è©¦: {test_id} ================")

    port_map = generate_docker_compose(config, work_dir)

    try:
        print(f"ğŸš€ å•Ÿå‹• Docker ç’°å¢ƒ (Dir: {work_dir})...")
        subprocess.run(["docker", "compose", "up", "-d"], cwd=work_dir, check=True)

        if wait_for_services(port_map, timeout=900):
            sorted_ports = sorted(port_map.keys())
            p_count = config.get("producers", 0)

            if config["type"] == "disaggregated":
                p_urls = ",".join([f"http://localhost:{p}/v1" for p in sorted_ports[:p_count]])
                c_urls = ",".join([f"http://localhost:{p}/v1" for p in sorted_ports[p_count:]])
            else:
                p_urls = ""
                c_urls = ",".join([f"http://localhost:{p}/v1" for p in sorted_ports])

            cmd = [
                "uv", "run", str(TESTER_SCRIPT),
                "--test-id", test_id,
                "--producers", p_urls,
                "--consumers", c_urls,
                "--output-dir", str(work_dir)
            ]

            print(f"ğŸ§ª åŸ·è¡Œæ¸¬è©¦è…³æœ¬...")
            subprocess.run(cmd, check=True)
        else:
            print("âš ï¸ æ¸¬è©¦ä¸­æ­¢ï¼šæœå‹™å•Ÿå‹•å¤±æ•—")

    except Exception as e:
        print(f"âŒ ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
    finally:
        print(f"ğŸ§¹ æ­£åœ¨æ¸…ç† {test_id}...")
        subprocess.run(["docker", "compose", "down"], cwd=work_dir)
        shm_path = Path(f"/dev/shm/lmcache_{test_id}")
        if shm_path.exists():
            shutil.rmtree(shm_path, ignore_errors=True)

if __name__ == "__main__":
    if not MODELS_DIR or not Path(MODELS_DIR).exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ¨¡å‹ç›®éŒ„ {MODELS_DIR} ä¸å­˜åœ¨ã€‚")
        exit(1)

    prepare_lmcache_source()

    for config in TEST_MATRIX:
        run_single_benchmark(config)
        time.sleep(10)
